{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ca1839f-4ee8-4c9d-b08e-ef6297f0f2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    " \n",
    "import keyboard\n",
    "import numpy as np\n",
    "import time\n",
    "env=gym.make('FrozenLake-v1', desc=None, is_slippery=False)\n",
    "env.reset()\n",
    "env.render()\n",
    "# format of the returnValue: (observation,reward, terminated, truncated, info)\n",
    "# observation (object)  - observed state\n",
    "# reward (float)        - reward that is the result of taking the action\n",
    "# terminated (bool)     - is it a terminal state\n",
    "# truncated (bool)      - it is not used in this lab (it repressents the max time for reaching successfully  the end of episode\n",
    "# info (dictionary)     - the transition probability\n",
    "def MonteCarloLearnStateValueFct(env,stateNumber,numberOfEpisodes,discountRate):\n",
    "     \n",
    "    # return for every state\n",
    "    ReturnOfState=np.zeros(stateNumber)\n",
    "    # number of visits of every state\n",
    " \n",
    "    numberVisitsOfState=np.zeros(stateNumber)\n",
    "     \n",
    "    # estimate of the value of each state\n",
    "    valueFunctionEstimate=np.zeros(stateNumber)\n",
    "     \n",
    "    # initialization of the current episode\n",
    "    for indexEpisode in range(numberOfEpisodes):\n",
    "        # this list stores visited states in the current episode\n",
    "        visitedStatesInEpisode=[]\n",
    "        # this list stores the return of each visited state in the current episode\n",
    "        rewardInVisitedState=[]\n",
    "        (currentState,prob)=env.reset()\n",
    "        print(\"currentState:\",currentState)\n",
    "        print(\"prob:\",prob)\n",
    "        visitedStatesInEpisode.append(currentState)\n",
    "         \n",
    "        print(\"Simulated episode\",indexEpisode)\n",
    "             \n",
    "        # Starting Montecarlo simulation \n",
    "        # we randomly generate actions and advance in the path\n",
    "        # when the terminal state is attained, the loop stops\n",
    "        advance=True\n",
    "        while advance:\n",
    "             \n",
    "            # select a random action\n",
    "            randomAction= env.action_space.sample()\n",
    "                        \n",
    "            # here we step and return the state, reward, and boolean denoting if the state is a terminal state\n",
    "            (currentState, currentReward, terminalState,_,_) = env.step(randomAction)          \n",
    "             \n",
    "            # append the reward\n",
    "            rewardInVisitedState.append(currentReward)\n",
    "             \n",
    "            # if the current state is NOT terminal state \n",
    "            if not terminalState:\n",
    "                visitedStatesInEpisode.append(currentState)   \n",
    "            # if the current state IS terminal state \n",
    "            else: \n",
    "                #break\n",
    "                advance=False\n",
    "\n",
    "        # END of the actual episode simulation\n",
    "\n",
    "        # how many states we visited in an episode    \n",
    "        numberOfVisitedStates=len(visitedStatesInEpisode)\n",
    "             \n",
    "        # the gain is equal to: Gt=R_{t+1}+\\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...\n",
    "        Gt=0\n",
    "        # we compute this quantity using a reverse \"range\":from len-1 until second argument +1, that is until 0\n",
    "       \n",
    "        for indexCurrentState in range(numberOfVisitedStates-1,-1,-1):\n",
    "                 \n",
    "            stateTmp=visitedStatesInEpisode[indexCurrentState] \n",
    "            returnTmp=rewardInVisitedState[indexCurrentState]\n",
    "               \n",
    "            # this is an intelligent way of summing the returns \n",
    "               \n",
    "            Gt=discountRate*Gt+returnTmp\n",
    "               \n",
    "            # below is the first visit implementation \n",
    "            # we note that the notation a[0:3], includes a[0],a[1],a[2] and it does NOT include a[3]\n",
    "            if stateTmp not in visitedStatesInEpisode[0:indexCurrentState]:\n",
    "                #  this state is visited in the episode\n",
    "                numberVisitsOfState[stateTmp]=numberVisitsOfState[stateTmp]+1\n",
    "                # add the sum for the state to the total sum for the same state\n",
    "                ReturnOfState[stateTmp]=ReturnOfState[stateTmp]+Gt\n",
    "             \n",
    "     \n",
    "    #offline update of the states' values \n",
    "    # END of the simulated episode \n",
    "     \n",
    "    # calculation of the final estimate of the state value \n",
    "    for indexSum in range(stateNumber):\n",
    "        if numberVisitsOfState[indexSum] !=0:\n",
    "            valueFunctionEstimate[indexSum]=ReturnOfState[indexSum]/numberVisitsOfState[indexSum]\n",
    "         \n",
    "    return valueFunctionEstimate           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "360f19e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MonteCarloLearnStateValueFctIncrementall(env,stateNumber,numberOfEpisodes,discountRate):\n",
    "\n",
    "    # convergence list \n",
    "    convergenceList=[]\n",
    "     \n",
    "    # return for every state\n",
    "    ReturnOfState=np.zeros(stateNumber)\n",
    "    # number of visits of every state\n",
    " \n",
    "    numberVisitsOfState=np.zeros(stateNumber)\n",
    "     \n",
    "    # estimate of the value of each state\n",
    "    valueFunctionEstimate=np.zeros(stateNumber)\n",
    "    # initialization of the current episode\n",
    "    for indexEpisode in range(numberOfEpisodes):\n",
    "        valueFunctionEstimate_old = np.copy(valueFunctionEstimate)\n",
    "        # this list stores visited states in the current episode\n",
    "        visitedStatesInEpisode=[]\n",
    "        # this list stores the return of each visited state in the current episode\n",
    "        rewardInVisitedState=[]\n",
    "        (currentState,prob)=env.reset()\n",
    "        print(\"currentState:\",currentState)\n",
    "        print(\"prob:\",prob)\n",
    "        visitedStatesInEpisode.append(currentState)\n",
    "         \n",
    "        print(\"Simulated episode\",indexEpisode)\n",
    "             \n",
    "        # Starting Montecarlo simulation \n",
    "        # we randomly generate actions and advance in the path\n",
    "        # when the terminal state is attained, the loop stops\n",
    "        advance=True\n",
    "        while advance:\n",
    "             \n",
    "            # select a random action\n",
    "            randomAction= env.action_space.sample()\n",
    "                        \n",
    "            # here we step and return the state, reward, and boolean denoting if the state is a terminal state\n",
    "            (currentState, currentReward, terminalState,_,_) = env.step(randomAction)          \n",
    "             \n",
    "            # append the reward\n",
    "            rewardInVisitedState.append(currentReward)\n",
    "             \n",
    "            # if the current state is NOT terminal state \n",
    "            if not terminalState:\n",
    "                visitedStatesInEpisode.append(currentState)   \n",
    "            # if the current state IS terminal state \n",
    "            else: \n",
    "                #break\n",
    "                advance=False\n",
    "\n",
    "        # END of the actual episode simulation\n",
    "\n",
    "        # how many states we visited in an episode    \n",
    "        numberOfVisitedStates=len(visitedStatesInEpisode)\n",
    "             \n",
    "        # the gain is equal to: Gt=R_{t+1}+\\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...\n",
    "        Gt=0\n",
    "        # we compute this quantity using a reverse \"range\":from len-1 until second argument +1, that is until 0\n",
    "       \n",
    "        for indexCurrentState in range(numberOfVisitedStates-1,-1,-1):\n",
    "                 \n",
    "            stateTmp=visitedStatesInEpisode[indexCurrentState] \n",
    "            returnTmp=rewardInVisitedState[indexCurrentState]\n",
    "               \n",
    "            # this is an intelligent way of summing the returns \n",
    "               \n",
    "            Gt=discountRate*Gt+returnTmp\n",
    "               \n",
    "            # below is the first visit implementation \n",
    "            # we note that the notation a[0:3], includes a[0],a[1],a[2] and it does NOT include a[3]\n",
    "            if stateTmp not in visitedStatesInEpisode[0:indexCurrentState]:\n",
    "                #  this state is visited in the episode\n",
    "                numberVisitsOfState[stateTmp]=numberVisitsOfState[stateTmp]+1\n",
    "                valueFunctionEstimate[stateTmp]+=(1/numberVisitsOfState)*(Gt-valueFunctionEstimate[stateTmp])\n",
    "        print(\"converge\",np.max(np.abs(valueFunctionEstimate-valueFunctionEstimate_old)))\n",
    "        convergenceList.append(np.max(np.abs(valueFunctionEstimate-valueFunctionEstimate_old)))\n",
    "\n",
    "         \n",
    "    return valueFunctionEstimate,convergenceList           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5cf816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def TDEvaluation(env,stateNumber,numberOfEpisodes,alpha,gamma):\n",
    "\n",
    "    # convergence list \n",
    "    convergenceList=[]\n",
    "     \n",
    "    # number of visits of every state\n",
    " \n",
    "    # estimate of the value of each state\n",
    "    valueFunctionEstimate=np.zeros(stateNumber)\n",
    "    valueFunctionEstimate_old=np.zeros(stateNumber)\n",
    "    # initialization of the current episode\n",
    "    for indexEpisode in range(numberOfEpisodes):\n",
    "        valueFunctionEstimate_old = np.copy(valueFunctionEstimate)\n",
    "\n",
    "        # this list stores the return of each visited state in the current episode\n",
    "\n",
    "        (currentState,prob)=env.reset()\n",
    "         \n",
    "        advance=True\n",
    "        while advance:\n",
    "             \n",
    "            # select a random action\n",
    "            randomAction= env.action_space.sample()\n",
    "                        \n",
    "            # here we step and return the state, reward, and boolean denoting if the state is a terminal state\n",
    "            (next_state, next_reward, terminalState,_,_) = env.step(randomAction)          \n",
    "             \n",
    "            # if the current state is NOT terminal state \n",
    "            if not terminalState:\n",
    "                valueFunctionEstimate[currentState]+=alpha*(next_reward+(gamma*valueFunctionEstimate[next_state]) - valueFunctionEstimate[currentState])\n",
    "                currentState=next_state\n",
    "            else: \n",
    "                valueFunctionEstimate[currentState]+=alpha*(next_reward+(gamma*valueFunctionEstimate[next_state]) - valueFunctionEstimate[currentState])\n",
    "                #break\n",
    "                advance=False\n",
    "\n",
    "        # END of the actual episode simulation\n",
    "\n",
    "        convergenceList.append(np.max(np.abs(valueFunctionEstimate-valueFunctionEstimate_old)))\n",
    "\n",
    "         \n",
    "    return valueFunctionEstimate,convergenceList           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63540f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q, state, epsilon,num_actions):\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        return np.random.randint(0, num_actions)  # Random action\n",
    "    else:\n",
    "        return np.argmax(Q[state])  # Greedy action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ccae7639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, Q, episodes=100):\n",
    "    total_reward = 0\n",
    "    successes = 0\n",
    "    num_actions = env.action_space.n # Get action space size\n",
    "    for _ in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        steps=0\n",
    "        max_steps = 100 # Prevent infinite loops in non-terminating policies\n",
    "\n",
    "        while not done and steps < max_steps:\n",
    "            # Use greedy action (epsilon=0)\n",
    "            action = np.argmax(Q[state]) # Assuming discrete state\n",
    "\n",
    "            step_result = env.step(action)\n",
    "            if len(step_result) == 5:\n",
    "                 next_state, reward, terminated, truncated, info = step_result\n",
    "                 done = terminated or truncated\n",
    "            else:\n",
    "                 next_state, reward, terminated, info = step_result\n",
    "                 done = terminated\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        if episode_reward > 0: # Assuming reward > 0 only for success in FrozenLake\n",
    "            successes += 1\n",
    "\n",
    "    avg_reward = total_reward / episodes\n",
    "    success_rate = successes / episodes\n",
    "    return avg_reward, success_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b63882cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sarsa(env,numberOfEpisodes,alpha,gamma,epsilon=0.1):\n",
    "    convergence=[]\n",
    "    Q = np.zeros((env.observation_space.n,env.action_space.n))\n",
    "    old_Q=Q.copy()\n",
    "    num_actions = env.action_space.n\n",
    "    \n",
    "    for indexEpisode in range(numberOfEpisodes):\n",
    "\n",
    "        (currentState,prob)=env.reset()\n",
    "\n",
    "        action = epsilon_greedy_policy(Q,state=currentState,epsilon=epsilon,num_actions=num_actions)\n",
    "\n",
    "        advance=True\n",
    "\n",
    "        while advance:\n",
    "\n",
    "\n",
    "            # here we step and return the state, reward, and boolean denoting if the state is a terminal state\n",
    "            (nex_state, reward, terminalState,_,_) = env.step(action)          \n",
    "            next_action = epsilon_greedy_policy(Q,state=nex_state,epsilon=epsilon,num_actions=num_actions)\n",
    "            \n",
    "            #add the numbers of steps to the reward\n",
    "            # SARSA update rule\n",
    "            Q[currentState, action] += alpha * (reward + gamma * Q[nex_state, next_action] - Q[currentState,action])\n",
    "            currentState = nex_state\n",
    "            action = next_action\n",
    "\n",
    "            if terminalState:\n",
    "              advance=False\n",
    "    \n",
    "        eval_interval = 500 # Evaluate every 500 episodes\n",
    "        if (indexEpisode + 1) % eval_interval == 0:\n",
    "          avg_reward, success_rate = evaluate_policy(env, Q, episodes=100)\n",
    "          convergence.append(success_rate)\n",
    "          print(f\"Episode {indexEpisode + 1}: Avg Reward (Test): {avg_reward:.3f}, Success Rate: {success_rate:.2f}, Epsilon: {epsilon:.4f}\")\n",
    "\n",
    "    plt.plot(range(eval_interval, numberOfEpisodes + 1, eval_interval), convergence)\n",
    "    plt.xlabel(f\"Episodes (x{eval_interval})\")\n",
    "    plt.ylabel(\"Total Q-Table Change\")\n",
    "    plt.title(\"SARSA Q-Table Convergence\")\n",
    "    plt.show()\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfc119d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(env,numberOfEpisodes,alpha,gamma,epsilon=0.1):\n",
    "    convergence=[]\n",
    "    Q = np.zeros((env.observation_space.n,env.action_space.n))\n",
    "    old_Q=Q.copy()\n",
    "    num_actions = env.action_space.n\n",
    "    \n",
    "    for indexEpisode in range(numberOfEpisodes):\n",
    "\n",
    "        (currentState,prob)=env.reset()\n",
    "\n",
    "        action = epsilon_greedy_policy(Q,state=currentState,epsilon=epsilon,num_actions=num_actions)\n",
    "\n",
    "        advance=True\n",
    "\n",
    "        while advance:\n",
    "\n",
    "\n",
    "            # here we step and return the state, reward, and boolean denoting if the state is a terminal state\n",
    "            (nex_state, reward, terminalState,_,_) = env.step(action)          \n",
    "            \n",
    "            #add the numbers of steps to the reward\n",
    "            # SARSA update rule\n",
    "            Q[currentState, action] += alpha * (reward + gamma * (np.max(Q[nex_state])) - Q[currentState,action])\n",
    "            currentState = nex_state\n",
    "            action= epsilon_greedy_policy(Q,state=currentState,epsilon=epsilon,num_actions=num_actions)\n",
    "\n",
    "            if terminalState:\n",
    "              advance=False\n",
    "    \n",
    "        eval_interval = 500 # Evaluate every 500 episodes\n",
    "        if (indexEpisode + 1) % eval_interval == 0:\n",
    "          avg_reward, success_rate = evaluate_policy(env, Q, episodes=100)\n",
    "          convergence.append(success_rate)\n",
    "          print(f\"Episode {indexEpisode + 1}: Avg Reward (Test): {avg_reward:.3f}, Success Rate: {success_rate:.2f}, Epsilon: {epsilon:.4f}\")\n",
    "\n",
    "    plt.plot(range(eval_interval, numberOfEpisodes + 1, eval_interval), convergence)\n",
    "    plt.xlabel(f\"Episodes (x{eval_interval})\")\n",
    "    plt.ylabel(\"Total Q-Table Change\")\n",
    "    plt.title(\"Q-learning Q-Table Convergence\")\n",
    "    plt.show()\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "546402a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_soft_policy(Q, state, epsilon, actions):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(actions)\n",
    "    else:\n",
    "        return np.argmax(Q[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d8d45f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonteCarloControl(env,stateNumber,numberOfEpisodes,alpha,gamma,epsilon=0.1):\n",
    "\n",
    "    Q = np.zeros((env.observation_space.n,env.action_space.n))\n",
    "\n",
    "\n",
    "    (current_state,prob) = env.reset()\n",
    "\n",
    "    # initialization of the current episode\n",
    "    for indexEpisode in range(numberOfEpisodes):\n",
    "\n",
    "        episode=[]\n",
    "        (current_state,_) = env.reset()\n",
    "\n",
    "        advance = True\n",
    "\n",
    "        while advance:\n",
    "            action= epsilon_soft_policy(Q,state=current_state,epsilon=epsilon,actions=list(range(env.action_space.n)))\n",
    "            (next_state, reward, terminalState,_,_) = env.step(action)          \n",
    "            episode.append((current_state,action,reward))\n",
    "            if terminalState:\n",
    "                advance=False\n",
    "            current_state = next_state\n",
    "        \n",
    "        G=0\n",
    "        visited = set()\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            if (state, action) not in visited:\n",
    "                G = reward + gamma * G\n",
    "                Q[state, action] += alpha * (G - Q[state, action])\n",
    "                visited.add((state, action))\n",
    "\n",
    "        \n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44bc13ed-c3ef-4103-9149-31b88634dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  visualization\n",
    "def DisplayGrid(valueFunction,reshapeDim,fileNameToSave):\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt  \n",
    "    ax = sns.heatmap(valueFunction.reshape(reshapeDim,reshapeDim),\n",
    "                     annot=True, square=True,\n",
    "                     cbar=False, cmap='Blues',\n",
    "                     xticklabels=False, yticklabels=False)\n",
    "    plt.savefig(fileNameToSave,dpi=600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2d3ad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06b7a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_frozen_lake(env, Q, num_episodes=5, max_steps_per_episode=100, pause_time=0.3):\n",
    "\n",
    "    # --- Episode Loop ---\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset environment for the new episode\n",
    "        # Newer gym returns (state, info), older just state\n",
    "        reset_result = env.reset()\n",
    "        state = reset_result[0] if isinstance(reset_result, tuple) else reset_result\n",
    "\n",
    "        done = False\n",
    "        truncated = False # Gymnasium uses truncated flag\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        print(f\"\\n--- Watching Episode {episode + 1} ---\")\n",
    "\n",
    "        # Loop within the episode\n",
    "        while not done and not truncated and steps < max_steps_per_episode:\n",
    "            # Render the current state *before* taking the action\n",
    "            env.render()\n",
    "            time.sleep(pause_time) # Pause to watch\n",
    "\n",
    "            # Choose the BEST action based on Q-table (greedy)\n",
    "            action = np.argmax(Q[state])\n",
    "\n",
    "            # Take the action\n",
    "            step_result = env.step(action)\n",
    "\n",
    "            # Unpack results - handle older gym (4 results) vs newer (5 results)\n",
    "            if len(step_result) == 5:\n",
    "                next_state, reward, terminated, truncated, info = step_result\n",
    "                done = terminated or truncated # Episode ends if terminated OR truncated\n",
    "            else: # Fallback for older gym versions\n",
    "                next_state, reward, terminated, info = step_result\n",
    "                done = terminated\n",
    "                truncated = False # Assume no truncation in older versions\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "            # Check if done right after step to show the final state render\n",
    "            if done or truncated:\n",
    "                 env.render() # Render the final state (goal or hole)\n",
    "                 time.sleep(pause_time * 2) # Longer pause at the end frame\n",
    "\n",
    "\n",
    "        # --- End of Episode ---\n",
    "        print(f\"Episode finished after {steps} steps.\")\n",
    "        if reward == 1.0 and terminated: # Check standard FrozenLake success condition\n",
    "            print(\" Outcome: Reached the Goal! Hooray!\")\n",
    "        elif terminated: # Terminated but not goal (fell in hole)\n",
    "            print(\" Outcome: Fell in a Hole! Oops!\")\n",
    "        elif truncated:\n",
    "            print(f\" Outcome: Truncated at {max_steps_per_episode} steps (maybe stuck?).\")\n",
    "        else: # Should not happen if loop condition is correct\n",
    "             print(f\" Outcome: Unknown (Total Reward: {total_reward})\")\n",
    "\n",
    "        time.sleep(1.5) # Pause between episodes\n",
    "\n",
    "    print(\"\\n--- Finished Watching ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b195a821-dd05-4f00-8fe1-c370d6d48e8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500: Avg Reward (Test): 0.000, Success Rate: 0.00, Epsilon: 0.1000\n",
      "Episode 1000: Avg Reward (Test): 0.000, Success Rate: 0.00, Epsilon: 0.1000\n",
      "Episode 1500: Avg Reward (Test): 0.000, Success Rate: 0.00, Epsilon: 0.1000\n",
      "Episode 2000: Avg Reward (Test): 0.000, Success Rate: 0.00, Epsilon: 0.1000\n",
      "Episode 2500: Avg Reward (Test): 0.000, Success Rate: 0.00, Epsilon: 0.1000\n",
      "Episode 3000: Avg Reward (Test): 0.000, Success Rate: 0.00, Epsilon: 0.1000\n",
      "Episode 3500: Avg Reward (Test): 0.000, Success Rate: 0.00, Epsilon: 0.1000\n",
      "Episode 4000: Avg Reward (Test): 0.000, Success Rate: 0.00, Epsilon: 0.1000\n",
      "Episode 4500: Avg Reward (Test): 0.000, Success Rate: 0.00, Epsilon: 0.1000\n",
      "Episode 5000: Avg Reward (Test): 0.000, Success Rate: 0.00, Epsilon: 0.1000\n",
      "Episode 5500: Avg Reward (Test): 0.000, Success Rate: 0.00, Epsilon: 0.1000\n",
      "Episode 6000: Avg Reward (Test): 0.000, Success Rate: 0.00, Epsilon: 0.1000\n",
      "Episode 6500: Avg Reward (Test): 0.000, Success Rate: 0.00, Epsilon: 0.1000\n",
      "Episode 7000: Avg Reward (Test): 0.000, Success Rate: 0.00, Epsilon: 0.1000\n",
      "Episode 7500: Avg Reward (Test): 0.000, Success Rate: 0.00, Epsilon: 0.1000\n",
      "Episode 8000: Avg Reward (Test): 0.000, Success Rate: 0.00, Epsilon: 0.1000\n",
      "Episode 8500: Avg Reward (Test): 0.000, Success Rate: 0.00, Epsilon: 0.1000\n",
      "Episode 9000: Avg Reward (Test): 0.000, Success Rate: 0.00, Epsilon: 0.1000\n",
      "Episode 9500: Avg Reward (Test): 0.000, Success Rate: 0.00, Epsilon: 0.1000\n",
      "Episode 10000: Avg Reward (Test): 0.000, Success Rate: 0.00, Epsilon: 0.1000\n",
      "Episode 10500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 11000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 11500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 12000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 12500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 13000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 13500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 14000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 14500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 15000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 15500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 16000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 16500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 17000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 17500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 18000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 18500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 19000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 19500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 20000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 20500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 21000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 21500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 22000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 22500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 23000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 23500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 24000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 24500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 25000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 25500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 26000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 26500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 27000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 27500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 28000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 28500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 29000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 29500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 30000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 30500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 31000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 31500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 32000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 32500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 33000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 33500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 34000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 34500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 35000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 35500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 36000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 36500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 37000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 37500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 38000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 38500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 39000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 39500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 40000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 40500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 41000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 41500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 42000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 42500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 43000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 43500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 44000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 44500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 45000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 45500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 46000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 46500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 47000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 47500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 48000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 48500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 49000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 49500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 50000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 50500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 51000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 51500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 52000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 52500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 53000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 53500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 54000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 54500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 55000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 55500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 56000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 56500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 57000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 57500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 58000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 58500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 59000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 59500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 60000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 60500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 61000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 61500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 62000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 62500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 63000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 63500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 64000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 64500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 65000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 65500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 66000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 66500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 67000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 67500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 68000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 68500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 69000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 69500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 70000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 70500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 71000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 71500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 72000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 72500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 73000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 73500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 74000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 74500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 75000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 75500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 76000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 76500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 77000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 77500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 78000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 78500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 79000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 79500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 80000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 80500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 81000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 81500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 82000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 82500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 83000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 83500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 84000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 84500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 85000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 85500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 86000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 86500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 87000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 87500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 88000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 88500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 89000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 89500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 90000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 90500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 91000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 91500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 92000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 92500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 93000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 93500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 94000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 94500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 95000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 95500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 96000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 96500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 97000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 97500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 98000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 98500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 99000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 99500: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n",
      "Episode 100000: Avg Reward (Test): 1.000, Success Rate: 1.00, Epsilon: 0.1000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# number of states in the environment\n",
    "stateNumber=env.observation_space.n\n",
    " \n",
    "# number of simulation episodes\n",
    "numberOfEpisodes=100000\n",
    "\n",
    "# initial policy is  uniformly random \n",
    "# there is an equal probability of choosing a particular action\n",
    "initialPolicy=(1/4)*np.ones((16,4))\n",
    " \n",
    "# discount rate\n",
    "alpha=0.1\n",
    "gamma = 0.99\n",
    "# estimate the state value (function) through Monte Carlo method\n",
    "#estimatedValuesMonteCarlo=MonteCarloLearnStateValueFct(env,stateNumber=stateNumber,numberOfEpisodes=numberOfEpisodes,discountRate=discountRate)\n",
    "#estimatedValuesMonteCarlo,convergenceList=MonteCarloLearnStateValueFctIncremental(env,stateNumber=stateNumber,numberOfEpisodes=numberOfEpisodes,discountRate=discountRate)\n",
    "\n",
    "# #TD evaluation:\n",
    "# td_estimation,convergenceList=TDEvaluation(env,stateNumber=stateNumber,numberOfEpisodes=numberOfEpisodes,alpha=alpha,gamma=gamma)\n",
    "\n",
    "# DisplayGrid(td_estimation,reshapeDim=4,fileNameToSave='td_estimation.png') \n",
    "\n",
    "#Sarsa\n",
    "#Q = Sarsa(env=env,alpha=alpha,gamma=gamma,epsilon=0.1,numberOfEpisodes=numberOfEpisodes) \n",
    "\n",
    "# #Monte Carlo control\n",
    "# Q = MonteCarloControl(env=env,alpha=alpha,gamma=gamma,numberOfEpisodes=numberOfEpisodes,stateNumber=stateNumber)\n",
    "\n",
    "#Q_learning\n",
    "Q = Q_learning(env=env,alpha=alpha,gamma=gamma,epsilon=0.1,numberOfEpisodes=numberOfEpisodes) \n",
    "\n",
    "env.close()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b65338b-4374-4801-ac5b-099a6bbb1e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Watching Episode 1 ---\n",
      "Episode finished after 6 steps.\n",
      " Outcome: Reached the Goal! Hooray!\n",
      "\n",
      "--- Watching Episode 2 ---\n",
      "Episode finished after 6 steps.\n",
      " Outcome: Reached the Goal! Hooray!\n",
      "\n",
      "--- Watching Episode 3 ---\n",
      "Episode finished after 6 steps.\n",
      " Outcome: Reached the Goal! Hooray!\n",
      "\n",
      "--- Watching Episode 4 ---\n",
      "Episode finished after 6 steps.\n",
      " Outcome: Reached the Goal! Hooray!\n",
      "\n",
      "--- Watching Episode 5 ---\n",
      "Episode finished after 6 steps.\n",
      " Outcome: Reached the Goal! Hooray!\n",
      "\n",
      "--- Finished Watching ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env_render = gym.make('FrozenLake-v1', desc=None, is_slippery=False,render_mode=\"human\")\n",
    "\n",
    "play_frozen_lake(env_render, Q, num_episodes=5, pause_time=0.1)\n",
    "\n",
    "env_render.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885afd7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cbbfd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
