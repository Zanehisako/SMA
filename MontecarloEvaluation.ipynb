{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ca1839f-4ee8-4c9d-b08e-ef6297f0f2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/gymnasium/envs/toy_text/frozen_lake.py:334: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    " \n",
    "import keyboard\n",
    "import numpy as np\n",
    "import time\n",
    "env=gym.make('FrozenLake-v1', desc=None, is_slippery=True)\n",
    "env.reset()\n",
    "env.render()\n",
    "# format of the returnValue: (observation,reward, terminated, truncated, info)\n",
    "# observation (object)  - observed state\n",
    "# reward (float)        - reward that is the result of taking the action\n",
    "# terminated (bool)     - is it a terminal state\n",
    "# truncated (bool)      - it is not used in this lab (it repressents the max time for reaching successfully  the end of episode\n",
    "# info (dictionary)     - the transition probability\n",
    "def MonteCarloLearnStateValueFct(env,stateNumber,numberOfEpisodes,discountRate):\n",
    "     \n",
    "    # return for every state\n",
    "    ReturnOfState=np.zeros(stateNumber)\n",
    "    # number of visits of every state\n",
    " \n",
    "    numberVisitsOfState=np.zeros(stateNumber)\n",
    "     \n",
    "    # estimate of the value of each state\n",
    "    valueFunctionEstimate=np.zeros(stateNumber)\n",
    "     \n",
    "    # initialization of the current episode\n",
    "    for indexEpisode in range(numberOfEpisodes):\n",
    "        # this list stores visited states in the current episode\n",
    "        visitedStatesInEpisode=[]\n",
    "        # this list stores the return of each visited state in the current episode\n",
    "        rewardInVisitedState=[]\n",
    "        (currentState,prob)=env.reset()\n",
    "        print(\"currentState:\",currentState)\n",
    "        print(\"prob:\",prob)\n",
    "        visitedStatesInEpisode.append(currentState)\n",
    "         \n",
    "        print(\"Simulated episode\",indexEpisode)\n",
    "             \n",
    "        # Starting Montecarlo simulation \n",
    "        # we randomly generate actions and advance in the path\n",
    "        # when the terminal state is attained, the loop stops\n",
    "        advance=True\n",
    "        while advance:\n",
    "             \n",
    "            # select a random action\n",
    "            randomAction= env.action_space.sample()\n",
    "                        \n",
    "            # here we step and return the state, reward, and boolean denoting if the state is a terminal state\n",
    "            (currentState, currentReward, terminalState,_,_) = env.step(randomAction)          \n",
    "             \n",
    "            # append the reward\n",
    "            rewardInVisitedState.append(currentReward)\n",
    "             \n",
    "            # if the current state is NOT terminal state \n",
    "            if not terminalState:\n",
    "                visitedStatesInEpisode.append(currentState)   \n",
    "            # if the current state IS terminal state \n",
    "            else: \n",
    "                #break\n",
    "                advance=False\n",
    "\n",
    "        # END of the actual episode simulation\n",
    "\n",
    "        # how many states we visited in an episode    \n",
    "        numberOfVisitedStates=len(visitedStatesInEpisode)\n",
    "             \n",
    "        # the gain is equal to: Gt=R_{t+1}+\\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...\n",
    "        Gt=0\n",
    "        # we compute this quantity using a reverse \"range\":from len-1 until second argument +1, that is until 0\n",
    "       \n",
    "        for indexCurrentState in range(numberOfVisitedStates-1,-1,-1):\n",
    "                 \n",
    "            stateTmp=visitedStatesInEpisode[indexCurrentState] \n",
    "            returnTmp=rewardInVisitedState[indexCurrentState]\n",
    "               \n",
    "            # this is an intelligent way of summing the returns \n",
    "               \n",
    "            Gt=discountRate*Gt+returnTmp\n",
    "               \n",
    "            # below is the first visit implementation \n",
    "            # we note that the notation a[0:3], includes a[0],a[1],a[2] and it does NOT include a[3]\n",
    "            if stateTmp not in visitedStatesInEpisode[0:indexCurrentState]:\n",
    "                #  this state is visited in the episode\n",
    "                numberVisitsOfState[stateTmp]=numberVisitsOfState[stateTmp]+1\n",
    "                # add the sum for the state to the total sum for the same state\n",
    "                ReturnOfState[stateTmp]=ReturnOfState[stateTmp]+Gt\n",
    "             \n",
    "     \n",
    "    #offline update of the states' values \n",
    "    # END of the simulated episode \n",
    "     \n",
    "    # calculation of the final estimate of the state value \n",
    "    for indexSum in range(stateNumber):\n",
    "        if numberVisitsOfState[indexSum] !=0:\n",
    "            valueFunctionEstimate[indexSum]=ReturnOfState[indexSum]/numberVisitsOfState[indexSum]\n",
    "         \n",
    "    return valueFunctionEstimate           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "360f19e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MonteCarloLearnStateValueFctIncrementall(env,stateNumber,numberOfEpisodes,discountRate):\n",
    "\n",
    "    # convergence list \n",
    "    convergenceList=[]\n",
    "     \n",
    "    # return for every state\n",
    "    ReturnOfState=np.zeros(stateNumber)\n",
    "    # number of visits of every state\n",
    " \n",
    "    numberVisitsOfState=np.zeros(stateNumber)\n",
    "     \n",
    "    # estimate of the value of each state\n",
    "    valueFunctionEstimate=np.zeros(stateNumber)\n",
    "    # initialization of the current episode\n",
    "    for indexEpisode in range(numberOfEpisodes):\n",
    "        valueFunctionEstimate_old = np.copy(valueFunctionEstimate)\n",
    "        # this list stores visited states in the current episode\n",
    "        visitedStatesInEpisode=[]\n",
    "        # this list stores the return of each visited state in the current episode\n",
    "        rewardInVisitedState=[]\n",
    "        (currentState,prob)=env.reset()\n",
    "        print(\"currentState:\",currentState)\n",
    "        print(\"prob:\",prob)\n",
    "        visitedStatesInEpisode.append(currentState)\n",
    "         \n",
    "        print(\"Simulated episode\",indexEpisode)\n",
    "             \n",
    "        # Starting Montecarlo simulation \n",
    "        # we randomly generate actions and advance in the path\n",
    "        # when the terminal state is attained, the loop stops\n",
    "        advance=True\n",
    "        while advance:\n",
    "             \n",
    "            # select a random action\n",
    "            randomAction= env.action_space.sample()\n",
    "                        \n",
    "            # here we step and return the state, reward, and boolean denoting if the state is a terminal state\n",
    "            (currentState, currentReward, terminalState,_,_) = env.step(randomAction)          \n",
    "             \n",
    "            # append the reward\n",
    "            rewardInVisitedState.append(currentReward)\n",
    "             \n",
    "            # if the current state is NOT terminal state \n",
    "            if not terminalState:\n",
    "                visitedStatesInEpisode.append(currentState)   \n",
    "            # if the current state IS terminal state \n",
    "            else: \n",
    "                #break\n",
    "                advance=False\n",
    "\n",
    "        # END of the actual episode simulation\n",
    "\n",
    "        # how many states we visited in an episode    \n",
    "        numberOfVisitedStates=len(visitedStatesInEpisode)\n",
    "             \n",
    "        # the gain is equal to: Gt=R_{t+1}+\\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...\n",
    "        Gt=0\n",
    "        # we compute this quantity using a reverse \"range\":from len-1 until second argument +1, that is until 0\n",
    "       \n",
    "        for indexCurrentState in range(numberOfVisitedStates-1,-1,-1):\n",
    "                 \n",
    "            stateTmp=visitedStatesInEpisode[indexCurrentState] \n",
    "            returnTmp=rewardInVisitedState[indexCurrentState]\n",
    "               \n",
    "            # this is an intelligent way of summing the returns \n",
    "               \n",
    "            Gt=discountRate*Gt+returnTmp\n",
    "               \n",
    "            # below is the first visit implementation \n",
    "            # we note that the notation a[0:3], includes a[0],a[1],a[2] and it does NOT include a[3]\n",
    "            if stateTmp not in visitedStatesInEpisode[0:indexCurrentState]:\n",
    "                #  this state is visited in the episode\n",
    "                numberVisitsOfState[stateTmp]=numberVisitsOfState[stateTmp]+1\n",
    "                valueFunctionEstimate[stateTmp]+=(1/numberVisitsOfState)*(Gt-valueFunctionEstimate[stateTmp])\n",
    "        print(\"converge\",np.max(np.abs(valueFunctionEstimate-valueFunctionEstimate_old)))\n",
    "        convergenceList.append(np.max(np.abs(valueFunctionEstimate-valueFunctionEstimate_old)))\n",
    "\n",
    "         \n",
    "    return valueFunctionEstimate,convergenceList           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5cf816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MonteCarloLearnStateValueFctIncremental(env,stateNumber,numberOfEpisodes,discountRate):\n",
    "\n",
    "    # convergence list \n",
    "    convergenceList=[]\n",
    "     \n",
    "    # number of visits of every state\n",
    " \n",
    "    numberVisitsOfState=np.zeros(stateNumber)\n",
    "     \n",
    "    # estimate of the value of each state\n",
    "    valueFunctionEstimate=np.zeros(stateNumber)\n",
    "    valueFunctionEstimate_old=np.zeros(stateNumber)\n",
    "    # initialization of the current episode\n",
    "    for indexEpisode in range(numberOfEpisodes):\n",
    "        valueFunctionEstimate_old = np.copy(valueFunctionEstimate)\n",
    "        # this list stores visited states in the current episode\n",
    "        visitedStatesInEpisode=[]\n",
    "        # this list stores the return of each visited state in the current episode\n",
    "        rewardInVisitedState=[]\n",
    "        (currentState,prob)=env.reset()\n",
    "        print(\"currentState:\",currentState)\n",
    "        print(\"prob:\",prob)\n",
    "        visitedStatesInEpisode.append(currentState)\n",
    "         \n",
    "        print(\"Simulated episode\",indexEpisode)\n",
    "             \n",
    "        # Starting Montecarlo simulation \n",
    "        # we randomly generate actions and advance in the path\n",
    "        # when the terminal state is attained, the loop stops\n",
    "        advance=True\n",
    "        while advance:\n",
    "             \n",
    "            # select a random action\n",
    "            randomAction= env.action_space.sample()\n",
    "                        \n",
    "            # here we step and return the state, reward, and boolean denoting if the state is a terminal state\n",
    "            (currentState, currentReward, terminalState,_,_) = env.step(randomAction)          \n",
    "             \n",
    "            # append the reward\n",
    "            rewardInVisitedState.append(currentReward)\n",
    "             \n",
    "            # if the current state is NOT terminal state \n",
    "            if not terminalState:\n",
    "                visitedStatesInEpisode.append(currentState)   \n",
    "            # if the current state IS terminal state \n",
    "            else: \n",
    "                #break\n",
    "                advance=False\n",
    "\n",
    "        # END of the actual episode simulation\n",
    "\n",
    "        # how many states we visited in an episode    \n",
    "        numberOfVisitedStates=len(visitedStatesInEpisode)\n",
    "             \n",
    "        # the gain is equal to: Gt=R_{t+1}+\\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...\n",
    "        Gt=0\n",
    "        # we compute this quantity using a reverse \"range\":from len-1 until second argument +1, that is until 0\n",
    "       \n",
    "        for indexCurrentState in range(numberOfVisitedStates-1,-1,-1):\n",
    "                 \n",
    "            stateTmp=visitedStatesInEpisode[indexCurrentState] \n",
    "            returnTmp=rewardInVisitedState[indexCurrentState]\n",
    "               \n",
    "            # this is an intelligent way of summing the returns \n",
    "               \n",
    "            Gt=discountRate*Gt+returnTmp\n",
    "               \n",
    "            # below is the first visit implementation \n",
    "            # we note that the notation a[0:3], includes a[0],a[1],a[2] and it does NOT include a[3]\n",
    "            if stateTmp not in visitedStatesInEpisode[0:indexCurrentState]:\n",
    "                #  this state is visited in the episode\n",
    "                numberVisitsOfState[stateTmp]=numberVisitsOfState[stateTmp]+1\n",
    "                valueFunctionEstimate[stateTmp]=valueFunctionEstimate[stateTmp]+(1/numberVisitsOfState[stateTmp])*(Gt-valueFunctionEstimate[stateTmp])\n",
    "\n",
    "        convergenceList.append(np.max(np.abs(valueFunctionEstimate-valueFunctionEstimate_old)))\n",
    "\n",
    "         \n",
    "    return valueFunctionEstimate,convergenceList           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63540f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q, state, epsilon,num_actions):\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        return np.random.randint(0, num_actions)  # Random action\n",
    "    else:\n",
    "        return np.argmax(Q[state])  # Greedy action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccae7639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, Q, episodes=100):\n",
    "    total_reward = 0\n",
    "    successes = 0\n",
    "    num_actions = env.action_space.n # Get action space size\n",
    "    for _ in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        steps=0\n",
    "        max_steps = 100 # Prevent infinite loops in non-terminating policies\n",
    "\n",
    "        while not done and steps < max_steps:\n",
    "            # Use greedy action (epsilon=0)\n",
    "            action = np.argmax(Q[state]) # Assuming discrete state\n",
    "\n",
    "            step_result = env.step(action)\n",
    "            if len(step_result) == 5:\n",
    "                 next_state, reward, terminated, truncated, info = step_result\n",
    "                 done = terminated or truncated\n",
    "            else:\n",
    "                 next_state, reward, terminated, info = step_result\n",
    "                 done = terminated\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "        total_reward += episode_reward\n",
    "        if episode_reward > 0: # Assuming reward > 0 only for success in FrozenLake\n",
    "            successes += 1\n",
    "\n",
    "    avg_reward = total_reward / episodes\n",
    "    success_rate = successes / episodes\n",
    "    return avg_reward, success_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b63882cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sarsa(env,numberOfEpisodes,alpha,gamma,epsilon=0.1):\n",
    "    convergence=[]\n",
    "    Q = np.zeros((env.observation_space.n,env.action_space.n))\n",
    "    old_Q=Q.copy()\n",
    "    num_actions = env.action_space.n\n",
    "    \n",
    "    for indexEpisode in range(numberOfEpisodes):\n",
    "\n",
    "        (currentState,prob)=env.reset()\n",
    "\n",
    "        action = epsilon_greedy_policy(Q,state=currentState,epsilon=epsilon,num_actions=num_actions)\n",
    "\n",
    "        advance=True\n",
    "\n",
    "        while advance:\n",
    "\n",
    "\n",
    "            # here we step and return the state, reward, and boolean denoting if the state is a terminal state\n",
    "            (nex_state, reward, terminalState,_,_) = env.step(action)          \n",
    "            next_action = epsilon_greedy_policy(Q,state=nex_state,epsilon=epsilon,num_actions=num_actions)\n",
    "\n",
    "              # SARSA update rule\n",
    "            Q[currentState, action] += alpha * (reward + gamma * Q[nex_state, next_action] - Q[currentState,action])\n",
    "            currentState = nex_state\n",
    "            action = next_action\n",
    "\n",
    "            if terminalState:\n",
    "              advance=False\n",
    "    \n",
    "        eval_interval = 500 # Evaluate every 500 episodes\n",
    "        if (indexEpisode + 1) % eval_interval == 0:\n",
    "          avg_reward, success_rate = evaluate_policy(env, Q, episodes=100)\n",
    "          print(f\"Episode {indexEpisode + 1}: Avg Reward (Test): {avg_reward:.3f}, Success Rate: {success_rate:.2f}, Epsilon: {epsilon:.4f}\")\n",
    "\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44bc13ed-c3ef-4103-9149-31b88634dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  visualization\n",
    "def DisplayGrid(valueFunction,reshapeDim,fileNameToSave):\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt  \n",
    "    ax = sns.heatmap(valueFunction.reshape(reshapeDim,reshapeDim),\n",
    "                     annot=True, square=True,\n",
    "                     cbar=False, cmap='Blues',\n",
    "                     xticklabels=False, yticklabels=False)\n",
    "    plt.savefig(fileNameToSave,dpi=600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2d3ad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06b7a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_frozen_lake(env, Q, num_episodes=5, max_steps_per_episode=100, pause_time=0.3):\n",
    "\n",
    "    # --- Episode Loop ---\n",
    "    for episode in range(num_episodes):\n",
    "        # Reset environment for the new episode\n",
    "        # Newer gym returns (state, info), older just state\n",
    "        reset_result = env.reset()\n",
    "        state = reset_result[0] if isinstance(reset_result, tuple) else reset_result\n",
    "\n",
    "        done = False\n",
    "        truncated = False # Gymnasium uses truncated flag\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        print(f\"\\n--- Watching Episode {episode + 1} ---\")\n",
    "\n",
    "        # Loop within the episode\n",
    "        while not done and not truncated and steps < max_steps_per_episode:\n",
    "            # Render the current state *before* taking the action\n",
    "            env.render()\n",
    "            time.sleep(pause_time) # Pause to watch\n",
    "\n",
    "            # Choose the BEST action based on Q-table (greedy)\n",
    "            action = np.argmax(Q[state])\n",
    "\n",
    "            # Take the action\n",
    "            step_result = env.step(action)\n",
    "\n",
    "            # Unpack results - handle older gym (4 results) vs newer (5 results)\n",
    "            if len(step_result) == 5:\n",
    "                next_state, reward, terminated, truncated, info = step_result\n",
    "                done = terminated or truncated # Episode ends if terminated OR truncated\n",
    "            else: # Fallback for older gym versions\n",
    "                next_state, reward, terminated, info = step_result\n",
    "                done = terminated\n",
    "                truncated = False # Assume no truncation in older versions\n",
    "\n",
    "            # Update state\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "            # Check if done right after step to show the final state render\n",
    "            if done or truncated:\n",
    "                 env.render() # Render the final state (goal or hole)\n",
    "                 time.sleep(pause_time * 2) # Longer pause at the end frame\n",
    "\n",
    "\n",
    "        # --- End of Episode ---\n",
    "        print(f\"Episode finished after {steps} steps.\")\n",
    "        if reward == 1.0 and terminated: # Check standard FrozenLake success condition\n",
    "            print(\" Outcome: Reached the Goal! Hooray!\")\n",
    "        elif terminated: # Terminated but not goal (fell in hole)\n",
    "            print(\" Outcome: Fell in a Hole! Oops!\")\n",
    "        elif truncated:\n",
    "            print(f\" Outcome: Truncated at {max_steps_per_episode} steps (maybe stuck?).\")\n",
    "        else: # Should not happen if loop condition is correct\n",
    "             print(f\" Outcome: Unknown (Total Reward: {total_reward})\")\n",
    "\n",
    "        time.sleep(1.5) # Pause between episodes\n",
    "\n",
    "    print(\"\\n--- Finished Watching ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b195a821-dd05-4f00-8fe1-c370d6d48e8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "Episode 500: Avg Reward (Test): 0.000, Success Rate: 0.00, Epsilon: 0.1000\n",
      "Episode 1000: Avg Reward (Test): 0.000, Success Rate: 0.00, Epsilon: 0.1000\n",
      "Episode 1500: Avg Reward (Test): 0.020, Success Rate: 0.02, Epsilon: 0.1000\n",
      "Episode 2000: Avg Reward (Test): 0.020, Success Rate: 0.02, Epsilon: 0.1000\n",
      "Episode 2500: Avg Reward (Test): 0.020, Success Rate: 0.02, Epsilon: 0.1000\n",
      "Episode 3000: Avg Reward (Test): 0.230, Success Rate: 0.23, Epsilon: 0.1000\n",
      "Episode 3500: Avg Reward (Test): 0.330, Success Rate: 0.33, Epsilon: 0.1000\n",
      "Episode 4000: Avg Reward (Test): 0.680, Success Rate: 0.68, Epsilon: 0.1000\n",
      "Episode 4500: Avg Reward (Test): 0.670, Success Rate: 0.67, Epsilon: 0.1000\n",
      "Episode 5000: Avg Reward (Test): 0.670, Success Rate: 0.67, Epsilon: 0.1000\n",
      "Episode 5500: Avg Reward (Test): 0.770, Success Rate: 0.77, Epsilon: 0.1000\n",
      "Episode 6000: Avg Reward (Test): 0.670, Success Rate: 0.67, Epsilon: 0.1000\n",
      "Episode 6500: Avg Reward (Test): 0.570, Success Rate: 0.57, Epsilon: 0.1000\n",
      "Episode 7000: Avg Reward (Test): 0.710, Success Rate: 0.71, Epsilon: 0.1000\n",
      "Episode 7500: Avg Reward (Test): 0.610, Success Rate: 0.61, Epsilon: 0.1000\n",
      "Episode 8000: Avg Reward (Test): 0.730, Success Rate: 0.73, Epsilon: 0.1000\n",
      "Episode 8500: Avg Reward (Test): 0.740, Success Rate: 0.74, Epsilon: 0.1000\n",
      "Episode 9000: Avg Reward (Test): 0.770, Success Rate: 0.77, Epsilon: 0.1000\n",
      "Episode 9500: Avg Reward (Test): 0.700, Success Rate: 0.70, Epsilon: 0.1000\n",
      "Episode 10000: Avg Reward (Test): 0.470, Success Rate: 0.47, Epsilon: 0.1000\n",
      "Episode 10500: Avg Reward (Test): 0.790, Success Rate: 0.79, Epsilon: 0.1000\n",
      "Episode 11000: Avg Reward (Test): 0.770, Success Rate: 0.77, Epsilon: 0.1000\n",
      "Episode 11500: Avg Reward (Test): 0.650, Success Rate: 0.65, Epsilon: 0.1000\n",
      "Episode 12000: Avg Reward (Test): 0.760, Success Rate: 0.76, Epsilon: 0.1000\n",
      "Episode 12500: Avg Reward (Test): 0.660, Success Rate: 0.66, Epsilon: 0.1000\n",
      "Episode 13000: Avg Reward (Test): 0.680, Success Rate: 0.68, Epsilon: 0.1000\n",
      "Episode 13500: Avg Reward (Test): 0.670, Success Rate: 0.67, Epsilon: 0.1000\n",
      "Episode 14000: Avg Reward (Test): 0.730, Success Rate: 0.73, Epsilon: 0.1000\n",
      "Episode 14500: Avg Reward (Test): 0.790, Success Rate: 0.79, Epsilon: 0.1000\n",
      "Episode 15000: Avg Reward (Test): 0.690, Success Rate: 0.69, Epsilon: 0.1000\n",
      "Episode 15500: Avg Reward (Test): 0.750, Success Rate: 0.75, Epsilon: 0.1000\n",
      "Episode 16000: Avg Reward (Test): 0.680, Success Rate: 0.68, Epsilon: 0.1000\n",
      "Episode 16500: Avg Reward (Test): 0.770, Success Rate: 0.77, Epsilon: 0.1000\n",
      "Episode 17000: Avg Reward (Test): 0.640, Success Rate: 0.64, Epsilon: 0.1000\n",
      "Episode 17500: Avg Reward (Test): 0.780, Success Rate: 0.78, Epsilon: 0.1000\n",
      "Episode 18000: Avg Reward (Test): 0.690, Success Rate: 0.69, Epsilon: 0.1000\n",
      "Episode 18500: Avg Reward (Test): 0.810, Success Rate: 0.81, Epsilon: 0.1000\n",
      "Episode 19000: Avg Reward (Test): 0.720, Success Rate: 0.72, Epsilon: 0.1000\n",
      "Episode 19500: Avg Reward (Test): 0.740, Success Rate: 0.74, Epsilon: 0.1000\n",
      "Episode 20000: Avg Reward (Test): 0.720, Success Rate: 0.72, Epsilon: 0.1000\n",
      "Episode 20500: Avg Reward (Test): 0.720, Success Rate: 0.72, Epsilon: 0.1000\n",
      "Episode 21000: Avg Reward (Test): 0.730, Success Rate: 0.73, Epsilon: 0.1000\n",
      "Episode 21500: Avg Reward (Test): 0.730, Success Rate: 0.73, Epsilon: 0.1000\n",
      "Episode 22000: Avg Reward (Test): 0.710, Success Rate: 0.71, Epsilon: 0.1000\n",
      "Episode 22500: Avg Reward (Test): 0.690, Success Rate: 0.69, Epsilon: 0.1000\n",
      "Episode 23000: Avg Reward (Test): 0.710, Success Rate: 0.71, Epsilon: 0.1000\n",
      "Episode 23500: Avg Reward (Test): 0.680, Success Rate: 0.68, Epsilon: 0.1000\n",
      "Episode 24000: Avg Reward (Test): 0.640, Success Rate: 0.64, Epsilon: 0.1000\n",
      "Episode 24500: Avg Reward (Test): 0.750, Success Rate: 0.75, Epsilon: 0.1000\n",
      "Episode 25000: Avg Reward (Test): 0.580, Success Rate: 0.58, Epsilon: 0.1000\n",
      "Episode 25500: Avg Reward (Test): 0.650, Success Rate: 0.65, Epsilon: 0.1000\n",
      "Episode 26000: Avg Reward (Test): 0.750, Success Rate: 0.75, Epsilon: 0.1000\n",
      "Episode 26500: Avg Reward (Test): 0.760, Success Rate: 0.76, Epsilon: 0.1000\n",
      "Episode 27000: Avg Reward (Test): 0.590, Success Rate: 0.59, Epsilon: 0.1000\n",
      "Episode 27500: Avg Reward (Test): 0.750, Success Rate: 0.75, Epsilon: 0.1000\n",
      "Episode 28000: Avg Reward (Test): 0.730, Success Rate: 0.73, Epsilon: 0.1000\n",
      "Episode 28500: Avg Reward (Test): 0.720, Success Rate: 0.72, Epsilon: 0.1000\n",
      "Episode 29000: Avg Reward (Test): 0.710, Success Rate: 0.71, Epsilon: 0.1000\n",
      "Episode 29500: Avg Reward (Test): 0.730, Success Rate: 0.73, Epsilon: 0.1000\n",
      "Episode 30000: Avg Reward (Test): 0.680, Success Rate: 0.68, Epsilon: 0.1000\n",
      "Episode 30500: Avg Reward (Test): 0.740, Success Rate: 0.74, Epsilon: 0.1000\n",
      "Episode 31000: Avg Reward (Test): 0.740, Success Rate: 0.74, Epsilon: 0.1000\n",
      "Episode 31500: Avg Reward (Test): 0.730, Success Rate: 0.73, Epsilon: 0.1000\n",
      "Episode 32000: Avg Reward (Test): 0.760, Success Rate: 0.76, Epsilon: 0.1000\n",
      "Episode 32500: Avg Reward (Test): 0.690, Success Rate: 0.69, Epsilon: 0.1000\n",
      "Episode 33000: Avg Reward (Test): 0.700, Success Rate: 0.70, Epsilon: 0.1000\n",
      "Episode 33500: Avg Reward (Test): 0.650, Success Rate: 0.65, Epsilon: 0.1000\n",
      "Episode 34000: Avg Reward (Test): 0.770, Success Rate: 0.77, Epsilon: 0.1000\n",
      "Episode 34500: Avg Reward (Test): 0.830, Success Rate: 0.83, Epsilon: 0.1000\n",
      "Episode 35000: Avg Reward (Test): 0.790, Success Rate: 0.79, Epsilon: 0.1000\n",
      "Episode 35500: Avg Reward (Test): 0.740, Success Rate: 0.74, Epsilon: 0.1000\n",
      "Episode 36000: Avg Reward (Test): 0.720, Success Rate: 0.72, Epsilon: 0.1000\n",
      "Episode 36500: Avg Reward (Test): 0.630, Success Rate: 0.63, Epsilon: 0.1000\n",
      "Episode 37000: Avg Reward (Test): 0.710, Success Rate: 0.71, Epsilon: 0.1000\n",
      "Episode 37500: Avg Reward (Test): 0.730, Success Rate: 0.73, Epsilon: 0.1000\n",
      "Episode 38000: Avg Reward (Test): 0.770, Success Rate: 0.77, Epsilon: 0.1000\n",
      "Episode 38500: Avg Reward (Test): 0.690, Success Rate: 0.69, Epsilon: 0.1000\n",
      "Episode 39000: Avg Reward (Test): 0.560, Success Rate: 0.56, Epsilon: 0.1000\n",
      "Episode 39500: Avg Reward (Test): 0.710, Success Rate: 0.71, Epsilon: 0.1000\n",
      "Episode 40000: Avg Reward (Test): 0.770, Success Rate: 0.77, Epsilon: 0.1000\n",
      "Episode 40500: Avg Reward (Test): 0.680, Success Rate: 0.68, Epsilon: 0.1000\n",
      "Episode 41000: Avg Reward (Test): 0.770, Success Rate: 0.77, Epsilon: 0.1000\n",
      "Episode 41500: Avg Reward (Test): 0.720, Success Rate: 0.72, Epsilon: 0.1000\n",
      "Episode 42000: Avg Reward (Test): 0.840, Success Rate: 0.84, Epsilon: 0.1000\n",
      "Episode 42500: Avg Reward (Test): 0.710, Success Rate: 0.71, Epsilon: 0.1000\n",
      "Episode 43000: Avg Reward (Test): 0.700, Success Rate: 0.70, Epsilon: 0.1000\n",
      "Episode 43500: Avg Reward (Test): 0.700, Success Rate: 0.70, Epsilon: 0.1000\n",
      "Episode 44000: Avg Reward (Test): 0.530, Success Rate: 0.53, Epsilon: 0.1000\n",
      "Episode 44500: Avg Reward (Test): 0.580, Success Rate: 0.58, Epsilon: 0.1000\n",
      "Episode 45000: Avg Reward (Test): 0.570, Success Rate: 0.57, Epsilon: 0.1000\n",
      "Episode 45500: Avg Reward (Test): 0.740, Success Rate: 0.74, Epsilon: 0.1000\n",
      "Episode 46000: Avg Reward (Test): 0.770, Success Rate: 0.77, Epsilon: 0.1000\n",
      "Episode 46500: Avg Reward (Test): 0.710, Success Rate: 0.71, Epsilon: 0.1000\n",
      "Episode 47000: Avg Reward (Test): 0.810, Success Rate: 0.81, Epsilon: 0.1000\n",
      "Episode 47500: Avg Reward (Test): 0.580, Success Rate: 0.58, Epsilon: 0.1000\n",
      "Episode 48000: Avg Reward (Test): 0.730, Success Rate: 0.73, Epsilon: 0.1000\n",
      "Episode 48500: Avg Reward (Test): 0.620, Success Rate: 0.62, Epsilon: 0.1000\n",
      "Episode 49000: Avg Reward (Test): 0.740, Success Rate: 0.74, Epsilon: 0.1000\n",
      "Episode 49500: Avg Reward (Test): 0.700, Success Rate: 0.70, Epsilon: 0.1000\n",
      "Episode 50000: Avg Reward (Test): 0.690, Success Rate: 0.69, Epsilon: 0.1000\n",
      "Episode 50500: Avg Reward (Test): 0.770, Success Rate: 0.77, Epsilon: 0.1000\n",
      "Episode 51000: Avg Reward (Test): 0.630, Success Rate: 0.63, Epsilon: 0.1000\n",
      "Episode 51500: Avg Reward (Test): 0.780, Success Rate: 0.78, Epsilon: 0.1000\n",
      "Episode 52000: Avg Reward (Test): 0.630, Success Rate: 0.63, Epsilon: 0.1000\n",
      "Episode 52500: Avg Reward (Test): 0.760, Success Rate: 0.76, Epsilon: 0.1000\n",
      "Episode 53000: Avg Reward (Test): 0.750, Success Rate: 0.75, Epsilon: 0.1000\n",
      "Episode 53500: Avg Reward (Test): 0.740, Success Rate: 0.74, Epsilon: 0.1000\n",
      "Episode 54000: Avg Reward (Test): 0.740, Success Rate: 0.74, Epsilon: 0.1000\n",
      "Episode 54500: Avg Reward (Test): 0.620, Success Rate: 0.62, Epsilon: 0.1000\n",
      "Episode 55000: Avg Reward (Test): 0.700, Success Rate: 0.70, Epsilon: 0.1000\n",
      "Episode 55500: Avg Reward (Test): 0.720, Success Rate: 0.72, Epsilon: 0.1000\n",
      "Episode 56000: Avg Reward (Test): 0.740, Success Rate: 0.74, Epsilon: 0.1000\n",
      "Episode 56500: Avg Reward (Test): 0.760, Success Rate: 0.76, Epsilon: 0.1000\n",
      "Episode 57000: Avg Reward (Test): 0.790, Success Rate: 0.79, Epsilon: 0.1000\n",
      "Episode 57500: Avg Reward (Test): 0.670, Success Rate: 0.67, Epsilon: 0.1000\n",
      "Episode 58000: Avg Reward (Test): 0.730, Success Rate: 0.73, Epsilon: 0.1000\n",
      "Episode 58500: Avg Reward (Test): 0.720, Success Rate: 0.72, Epsilon: 0.1000\n",
      "Episode 59000: Avg Reward (Test): 0.700, Success Rate: 0.70, Epsilon: 0.1000\n",
      "Episode 59500: Avg Reward (Test): 0.700, Success Rate: 0.70, Epsilon: 0.1000\n",
      "Episode 60000: Avg Reward (Test): 0.540, Success Rate: 0.54, Epsilon: 0.1000\n",
      "Episode 60500: Avg Reward (Test): 0.820, Success Rate: 0.82, Epsilon: 0.1000\n",
      "Episode 61000: Avg Reward (Test): 0.740, Success Rate: 0.74, Epsilon: 0.1000\n",
      "Episode 61500: Avg Reward (Test): 0.720, Success Rate: 0.72, Epsilon: 0.1000\n",
      "Episode 62000: Avg Reward (Test): 0.660, Success Rate: 0.66, Epsilon: 0.1000\n",
      "Episode 62500: Avg Reward (Test): 0.700, Success Rate: 0.70, Epsilon: 0.1000\n",
      "Episode 63000: Avg Reward (Test): 0.780, Success Rate: 0.78, Epsilon: 0.1000\n",
      "Episode 63500: Avg Reward (Test): 0.680, Success Rate: 0.68, Epsilon: 0.1000\n",
      "Episode 64000: Avg Reward (Test): 0.670, Success Rate: 0.67, Epsilon: 0.1000\n",
      "Episode 64500: Avg Reward (Test): 0.780, Success Rate: 0.78, Epsilon: 0.1000\n",
      "Episode 65000: Avg Reward (Test): 0.760, Success Rate: 0.76, Epsilon: 0.1000\n",
      "Episode 65500: Avg Reward (Test): 0.750, Success Rate: 0.75, Epsilon: 0.1000\n",
      "Episode 66000: Avg Reward (Test): 0.700, Success Rate: 0.70, Epsilon: 0.1000\n",
      "Episode 66500: Avg Reward (Test): 0.780, Success Rate: 0.78, Epsilon: 0.1000\n",
      "Episode 67000: Avg Reward (Test): 0.830, Success Rate: 0.83, Epsilon: 0.1000\n",
      "Episode 67500: Avg Reward (Test): 0.650, Success Rate: 0.65, Epsilon: 0.1000\n",
      "Episode 68000: Avg Reward (Test): 0.670, Success Rate: 0.67, Epsilon: 0.1000\n",
      "Episode 68500: Avg Reward (Test): 0.710, Success Rate: 0.71, Epsilon: 0.1000\n",
      "Episode 69000: Avg Reward (Test): 0.460, Success Rate: 0.46, Epsilon: 0.1000\n",
      "Episode 69500: Avg Reward (Test): 0.810, Success Rate: 0.81, Epsilon: 0.1000\n",
      "Episode 70000: Avg Reward (Test): 0.750, Success Rate: 0.75, Epsilon: 0.1000\n",
      "Episode 70500: Avg Reward (Test): 0.710, Success Rate: 0.71, Epsilon: 0.1000\n",
      "Episode 71000: Avg Reward (Test): 0.840, Success Rate: 0.84, Epsilon: 0.1000\n",
      "Episode 71500: Avg Reward (Test): 0.700, Success Rate: 0.70, Epsilon: 0.1000\n",
      "Episode 72000: Avg Reward (Test): 0.750, Success Rate: 0.75, Epsilon: 0.1000\n",
      "Episode 72500: Avg Reward (Test): 0.770, Success Rate: 0.77, Epsilon: 0.1000\n",
      "Episode 73000: Avg Reward (Test): 0.590, Success Rate: 0.59, Epsilon: 0.1000\n",
      "Episode 73500: Avg Reward (Test): 0.750, Success Rate: 0.75, Epsilon: 0.1000\n",
      "Episode 74000: Avg Reward (Test): 0.700, Success Rate: 0.70, Epsilon: 0.1000\n",
      "Episode 74500: Avg Reward (Test): 0.720, Success Rate: 0.72, Epsilon: 0.1000\n",
      "Episode 75000: Avg Reward (Test): 0.760, Success Rate: 0.76, Epsilon: 0.1000\n",
      "Episode 75500: Avg Reward (Test): 0.740, Success Rate: 0.74, Epsilon: 0.1000\n",
      "Episode 76000: Avg Reward (Test): 0.770, Success Rate: 0.77, Epsilon: 0.1000\n",
      "Episode 76500: Avg Reward (Test): 0.720, Success Rate: 0.72, Epsilon: 0.1000\n",
      "Episode 77000: Avg Reward (Test): 0.710, Success Rate: 0.71, Epsilon: 0.1000\n",
      "Episode 77500: Avg Reward (Test): 0.670, Success Rate: 0.67, Epsilon: 0.1000\n",
      "Episode 78000: Avg Reward (Test): 0.620, Success Rate: 0.62, Epsilon: 0.1000\n",
      "Episode 78500: Avg Reward (Test): 0.650, Success Rate: 0.65, Epsilon: 0.1000\n",
      "Episode 79000: Avg Reward (Test): 0.690, Success Rate: 0.69, Epsilon: 0.1000\n",
      "Episode 79500: Avg Reward (Test): 0.680, Success Rate: 0.68, Epsilon: 0.1000\n",
      "Episode 80000: Avg Reward (Test): 0.720, Success Rate: 0.72, Epsilon: 0.1000\n",
      "Episode 80500: Avg Reward (Test): 0.660, Success Rate: 0.66, Epsilon: 0.1000\n",
      "Episode 81000: Avg Reward (Test): 0.640, Success Rate: 0.64, Epsilon: 0.1000\n",
      "Episode 81500: Avg Reward (Test): 0.770, Success Rate: 0.77, Epsilon: 0.1000\n",
      "Episode 82000: Avg Reward (Test): 0.740, Success Rate: 0.74, Epsilon: 0.1000\n",
      "Episode 82500: Avg Reward (Test): 0.740, Success Rate: 0.74, Epsilon: 0.1000\n",
      "Episode 83000: Avg Reward (Test): 0.740, Success Rate: 0.74, Epsilon: 0.1000\n",
      "Episode 83500: Avg Reward (Test): 0.700, Success Rate: 0.70, Epsilon: 0.1000\n",
      "Episode 84000: Avg Reward (Test): 0.750, Success Rate: 0.75, Epsilon: 0.1000\n",
      "Episode 84500: Avg Reward (Test): 0.750, Success Rate: 0.75, Epsilon: 0.1000\n",
      "Episode 85000: Avg Reward (Test): 0.680, Success Rate: 0.68, Epsilon: 0.1000\n",
      "Episode 85500: Avg Reward (Test): 0.800, Success Rate: 0.80, Epsilon: 0.1000\n",
      "Episode 86000: Avg Reward (Test): 0.720, Success Rate: 0.72, Epsilon: 0.1000\n",
      "Episode 86500: Avg Reward (Test): 0.640, Success Rate: 0.64, Epsilon: 0.1000\n",
      "Episode 87000: Avg Reward (Test): 0.620, Success Rate: 0.62, Epsilon: 0.1000\n",
      "Episode 87500: Avg Reward (Test): 0.690, Success Rate: 0.69, Epsilon: 0.1000\n",
      "Episode 88000: Avg Reward (Test): 0.630, Success Rate: 0.63, Epsilon: 0.1000\n",
      "Episode 88500: Avg Reward (Test): 0.750, Success Rate: 0.75, Epsilon: 0.1000\n",
      "Episode 89000: Avg Reward (Test): 0.640, Success Rate: 0.64, Epsilon: 0.1000\n",
      "Episode 89500: Avg Reward (Test): 0.680, Success Rate: 0.68, Epsilon: 0.1000\n",
      "Episode 90000: Avg Reward (Test): 0.710, Success Rate: 0.71, Epsilon: 0.1000\n",
      "Episode 90500: Avg Reward (Test): 0.780, Success Rate: 0.78, Epsilon: 0.1000\n",
      "Episode 91000: Avg Reward (Test): 0.740, Success Rate: 0.74, Epsilon: 0.1000\n",
      "Episode 91500: Avg Reward (Test): 0.750, Success Rate: 0.75, Epsilon: 0.1000\n",
      "Episode 92000: Avg Reward (Test): 0.770, Success Rate: 0.77, Epsilon: 0.1000\n",
      "Episode 92500: Avg Reward (Test): 0.740, Success Rate: 0.74, Epsilon: 0.1000\n",
      "Episode 93000: Avg Reward (Test): 0.770, Success Rate: 0.77, Epsilon: 0.1000\n",
      "Episode 93500: Avg Reward (Test): 0.660, Success Rate: 0.66, Epsilon: 0.1000\n",
      "Episode 94000: Avg Reward (Test): 0.470, Success Rate: 0.47, Epsilon: 0.1000\n",
      "Episode 94500: Avg Reward (Test): 0.710, Success Rate: 0.71, Epsilon: 0.1000\n",
      "Episode 95000: Avg Reward (Test): 0.470, Success Rate: 0.47, Epsilon: 0.1000\n",
      "Episode 95500: Avg Reward (Test): 0.670, Success Rate: 0.67, Epsilon: 0.1000\n",
      "Episode 96000: Avg Reward (Test): 0.510, Success Rate: 0.51, Epsilon: 0.1000\n",
      "Episode 96500: Avg Reward (Test): 0.720, Success Rate: 0.72, Epsilon: 0.1000\n",
      "Episode 97000: Avg Reward (Test): 0.760, Success Rate: 0.76, Epsilon: 0.1000\n",
      "Episode 97500: Avg Reward (Test): 0.780, Success Rate: 0.78, Epsilon: 0.1000\n",
      "Episode 98000: Avg Reward (Test): 0.650, Success Rate: 0.65, Epsilon: 0.1000\n",
      "Episode 98500: Avg Reward (Test): 0.790, Success Rate: 0.79, Epsilon: 0.1000\n",
      "Episode 99000: Avg Reward (Test): 0.680, Success Rate: 0.68, Epsilon: 0.1000\n",
      "Episode 99500: Avg Reward (Test): 0.780, Success Rate: 0.78, Epsilon: 0.1000\n",
      "Episode 100000: Avg Reward (Test): 0.660, Success Rate: 0.66, Epsilon: 0.1000\n"
     ]
    }
   ],
   "source": [
    "# number of states in the environment\n",
    "stateNumber=env.observation_space.n\n",
    "print(env.observation_space.n)\n",
    " \n",
    "# number of simulation episodes\n",
    "numberOfEpisodes=100000\n",
    "\n",
    "# initial policy is  uniformly random \n",
    "# there is an equal probability of choosing a particular action\n",
    "initialPolicy=(1/4)*np.ones((16,4))\n",
    " \n",
    "# discount rate\n",
    "alpha=0.05\n",
    "gamma = 0.99\n",
    "# estimate the state value (function) through Monte Carlo method\n",
    "#estimatedValuesMonteCarlo=MonteCarloLearnStateValueFct(env,stateNumber=stateNumber,numberOfEpisodes=numberOfEpisodes,discountRate=discountRate)\n",
    "#estimatedValuesMonteCarlo,convergenceList=MonteCarloLearnStateValueFctIncremental(env,stateNumber=stateNumber,numberOfEpisodes=numberOfEpisodes,discountRate=discountRate)\n",
    "#DisplayGrid(estimatedValuesMonteCarlo,reshapeDim=4,fileNameToSave='monteCarloEstimated.png') \n",
    "Q = Sarsa(env=env,alpha=alpha,gamma=gamma,epsilon=0.1,numberOfEpisodes=numberOfEpisodes) \n",
    "\n",
    "env.close()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b65338b-4374-4801-ac5b-099a6bbb1e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Watching Episode 1 ---\n",
      "Episode finished after 56 steps.\n",
      " Outcome: Reached the Goal! Hooray!\n",
      "\n",
      "--- Watching Episode 2 ---\n",
      "Episode finished after 42 steps.\n",
      " Outcome: Reached the Goal! Hooray!\n",
      "\n",
      "--- Watching Episode 3 ---\n",
      "Episode finished after 22 steps.\n",
      " Outcome: Reached the Goal! Hooray!\n",
      "\n",
      "--- Watching Episode 4 ---\n",
      "Episode finished after 67 steps.\n",
      " Outcome: Fell in a Hole! Oops!\n",
      "\n",
      "--- Watching Episode 5 ---\n",
      "Episode finished after 10 steps.\n",
      " Outcome: Reached the Goal! Hooray!\n",
      "\n",
      "--- Finished Watching ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env_render = gym.make(\"FrozenLake-v1\",render_mode=\"human\")\n",
    "\n",
    "play_frozen_lake(env_render, Q, num_episodes=5, pause_time=0.1)\n",
    "\n",
    "env_render.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885afd7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cbbfd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
